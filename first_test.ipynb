{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from huggingface_hub import InferenceClient\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#os.environ[\"HF_TOKEN\"]=\n",
    "HF_TOKEN=\"something\"\n",
    "client = InferenceClient(api_key=HF_TOKEN)#\n",
    "#srun -N1 --gpus-per-node=1 -t 500 --pty bash\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m      6\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "#example \n",
    "output = client.text_generation(\n",
    "    \"The capital of France is\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model_transformer(name: str,device=\"mps\"):\n",
    "    # Check if model is already saved on disk\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available(\n",
    "                                ) else \"cpu\") if device is None else device\n",
    "    model_name=\"models/\"+name\n",
    "    if os.path.exists(model_name):\n",
    "        print('yep')\n",
    "        model = SentenceTransformer(model_name, device=device,token=HF_TOKEN,\n",
    "                                       trust_remote_code=True)\n",
    "    else:\n",
    "        model_name=name\n",
    "        # Save the model to disk\n",
    "        model = SentenceTransformer(model_name, device=device,token=HF_TOKEN,\n",
    "                                       trust_remote_code=True)\n",
    "        model.save(\"models/\"+name)\n",
    "    return model\n",
    "transModel = model_transformer(\"Alibaba-NLP/gte-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "from requests.exceptions import HTTPError\n",
    "client = InferenceClient(api_key=HF_TOKEN\n",
    "                         )\n",
    "def call_llm(messages: str=None, \n",
    "             ) -> str:\n",
    "    \n",
    "    output = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    max_tokens=265, \n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    stream=False,\n",
    "    seed=random.randint(0, 10000000)\n",
    "    ).choices[0].message.content\n",
    "\n",
    "#call_ret\n",
    "    return str(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a helpful assistant that generates a only SQL code. My ultimate goal is to discover as many diverse SQL as possible\n",
    "The user will provide: Provided Queries:  SELECT... Table(s) Description:\n",
    "\n",
    "Your Task\n",
    "- Generate \"one\" SQL query that is distinct from the user's queries and make it very diffrent than user's queries.\n",
    "- Ensure the query is meaningful and practical for real-world use cases.\n",
    "- Output only the SQL code**—no explanations, comments, or additional text or \"```sql\".\n",
    "\n",
    "never ever start with \"```sql\"\n",
    "\n",
    "Output Query:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an AI SQL query generator placeholders. Your goal is to generate **one diverse and meaningful SQL query**.\n",
    "\n",
    "\n",
    "### Input:\n",
    "- **Existing Queries**: SELECT...\n",
    "- **Table Structure**: General description.\n",
    "\n",
    "### Your Task:\n",
    "- Generate **one distinct SQL query** that is **structurally different** from the provided queries but still practical.\n",
    "- Use **varied SQL techniques** (e.g., JOINs, aggregation, subqueries, window functions).\n",
    "- Ensure the query is **syntactically correct** and **real-world applicable**.\n",
    "\n",
    "\n",
    "### Output:\n",
    "- **Return only the SQL query** (no explanations, comments, or extra text).\n",
    "- **Example Output:**  \n",
    "  ```sql\n",
    "  SELECT [col], COUNT(DISTINCT [value]) FROM [table] GROUP BY [col];\n",
    "- Maintain **general placeholders**:\n",
    "  - `[table]` for table names\n",
    "  - `[col]` for column names\n",
    "  - `[value]` for values\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "\n",
    "class SkillLibrary:\n",
    "    def __init__(self, library_path=\"skills.json\"):\n",
    "        self.library_path = library_path\n",
    "        # Load from disk or init empty\n",
    "        if os.path.exists(library_path):\n",
    "            with open(library_path, \"rb\", encoding=\"utf-8\") as f:\n",
    "                self.skills = json.load(f)\n",
    "        else:\n",
    "            self.skills = {}\n",
    "\n",
    "        dim = 1024  #  the vector dimension \n",
    "        if len(self.skills)==0:\n",
    "            self.vect_index = faiss.IndexFlatIP(dim)# ini\n",
    "\n",
    "        else :self.vect_index = faiss.IndexFlatIP(dim)# to be done \n",
    "\n",
    "        self.selected_index=[] # list of random selected index of sql skills \n",
    "        self.selected_ret_index = [] # list of retrieved selected index of sql skills ]\n",
    "    def __repr__(self):\n",
    "        return self.skills\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skills)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.library_path, \"wb\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.skills, f, indent=2)\n",
    "    \n",
    "    def get_sql(self,random_=True,num_q=5):\n",
    "        n_skill=len(self.skills)\n",
    "        if n_skill==0:\n",
    "            return [\"SELECT * \\nFROM [table];\" ]\n",
    "        if random_: \n",
    "            self.selected_index = random.sample(list(self.skills.keys()),k=min(num_q,n_skill\n",
    "                                                                )\n",
    "                                                ) \n",
    "            selected_sql = [self.skills[i][\"sql\"] for  i in  self.selected_index]\n",
    "        else:\n",
    "            n_skill-=1\n",
    "            selected_sql = [ self.skills[i] for i in range(n_skill,n_skill-num_q,-1) ]\n",
    "            self.selected_index = list(range(n_skill,n_skill-num_q,-1) )\n",
    "        \n",
    "        return selected_sql\n",
    "\n",
    "    def add_skill(self, sql: str, embedding_vec: List[float],python_func:str=None,save:bool=False)-> None:\n",
    "        \"\"\"\n",
    "        Add a new skill to the library with minimal info (only SQL).\n",
    "        \"\"\"\n",
    "        self.skills[len(self.skills)] = {\n",
    "            \"sql\" : sql,\n",
    "            \"embedding\": embedding_vec,\n",
    "            \"python_func\": python_func\n",
    "        }\n",
    "        self.vect_index.add(embedding_vec)\n",
    "        if save or len(self.skills)%100==0:\n",
    "            self.save()\n",
    "    \n",
    "    def find_similar(self, embedding_vec: List[float], top_k: int = 10, throushold:int=.9,) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return up to top_k skill names sorted by similarity (desc).\n",
    "        \"\"\"\n",
    "        # If skill library is empty, return empty\n",
    "        if not self.skills:\n",
    "            return []\n",
    "        \n",
    "        self.selected_ret_index=[]\n",
    "        # Compute similarity\n",
    "        sims, ret_index = self.vect_index.search(embedding_vec, k=top_k)\n",
    "        sims = sims[0]# because only one query\n",
    "        self.selected_ret_index=ret_index[0]# because only one query\n",
    "        self.selected_ret_index = [ self.selected_ret_index[i] for i in range(len(sims)\n",
    "                                                  )  if (sims[i] > throushold )]\n",
    "        ret_sql = [ self.skills[i]['sql'] for i in self.selected_ret_index  ] if len(self.selected_ret_index)>0 else []\n",
    "        \n",
    "        return ret_sql\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def get_table_info(self,database_name:str='database.sqlite')-> str:\n",
    "        \"\"\"\n",
    "        return description of dataset\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cursor = conn.cursor()\n",
    "        table_name=cursor.execute(\"SELECT name FROM sqlite_master\").fetchall()[0][0]\n",
    "        columns_info = cursor.execute(f\"PRAGMA table_info({table_name});\").fetchall()\n",
    "        table_description=f\"Table name :{table_name}\\n(column_index, column_name, data_type, not_null, default_value, primary_key)\\n{columns_info}\"\n",
    "        return table_description\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def test_sql_executable(sql_text: str, test_db_path=\":memory:\") -> bool:\n",
    "    \"\"\"\n",
    "    Quick check: parse & run the SQL in a small SQLite DB. If it fails, return False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(test_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # We run the SQL. If the SQL is malformed or has some error, it fails.\n",
    "        cursor.execute(sql_text)  # or .executescript for multi-statement\n",
    "        # Optionally fetch or commit\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "class SQLCurriculumAgent():\n",
    "    \"\"\"\n",
    "    Inspired by 'Voyager' curriculum.\n",
    "    - For each 'round', propose N new SQL queries (diverse and complex).\n",
    "    - Validate each query. If invalid, we revise. If too similar, we revise.\n",
    "    - Then store it in skill library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #messages: list,\n",
    "        skill_library: SkillLibrary,\n",
    "        max_retries: int = 5,\n",
    "        transModel=None\n",
    "    ):\n",
    "        self.skill_library = skill_library\n",
    "        #self.sim_threshold = similarity_threshold\n",
    "        self.max_retries = max_retries\n",
    "        self.messages=None\n",
    "        self.path_table=None\n",
    "    \n",
    "        self.model = transModel #model_transformer(\"Alibaba-NLP/gte-large-en-v1.5\")\n",
    "\n",
    "\n",
    "    def prompt_messages(self, SYSTEM_PROMPT,table_description):\n",
    "        sqls= self.skill_library.get_sql(random_=True)\n",
    "        self.messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"**Existing Queries**: {sqls } \\n**Table Description**: {table_description}\"},\n",
    "            ]\n",
    "\n",
    "    def create_info_table(self,path_table):\n",
    "        if path_table == self.path_table: return\n",
    "        self.path_table=path_table\n",
    "        self.table_info= self.skill_library.get_table_info(path_table)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.skill_library.skills)\n",
    "    \n",
    "    \n",
    "    def propose_sql_queries(self,path_table,SYSTEM_PROMPT) -> List[str]:\n",
    "        \"\"\"\n",
    "        Use your LLM to propose a diverse SQL query.\n",
    "        \"\"\"\n",
    "        self.create_info_table(path_table)#if get new table info\n",
    "\n",
    "        if not self.messages :\n",
    "            self.prompt_messages(SYSTEM_PROMPT,self.table_info)\n",
    "        # retrun one or more queires \n",
    "        try:\n",
    "            query =  call_llm(self.messages)\n",
    "        except HTTPError as e:\n",
    "            print(\"HTTPError\")\n",
    "            time.sleep(3)\n",
    "            query =  call_llm(self.messages)\n",
    "        #some preProsses\n",
    "        query_embed = self.compute_embedding(query)\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": query})\n",
    "        retrieve_skill=self.skill_library.find_similar( query_embed,self.max_retries )\n",
    "        if len(retrieve_skill)>0:\n",
    "            print(\"\\n\")\n",
    "            print(\"***********************************************************\")\n",
    "            print(f\"Retrrived and query: {query}\")\n",
    "            print(f\"is similar to existing queries: {retrieve_skill}\")\n",
    "            print(\"***********************************************************\")\n",
    "            print(\"\\n\")\n",
    "            self.messages.append({f\"role\": \"user\", \"content\": \"Not good because the previously generated query is very similar to the following SQL queries: {retrieve_skill}\"})\n",
    "            query,query_embed = self.propose_sql_queries(path_table,SYSTEM_PROMPT)\n",
    "        if len(self)>0:\n",
    "            prev=self.skill_library.skills[len(self)-1]\n",
    "            sec_cond= (np.dot(query_embed, prev[\"embedding\"].T)/(np.linalg.norm(query_embed)*np.linalg.norm(prev[\"embedding\"])))[0][0]\n",
    "            print(sec_cond)\n",
    "            if sec_cond < .6:\n",
    "                print(\"\\n\")\n",
    "                print(\"***********************************************************\")\n",
    "                print(f\"Seconde err and query: {query}\")\n",
    "                print(f\"is not similar at all to to existing queries: {prev['sql']}\")\n",
    "                print(\"***********************************************************\")\n",
    "                print(\"\\n\")\n",
    "                self.messages.append({\"role\": \"user\", \"content\": f\"Not good because the previously generated query is incorrect or too different from the previous SQL query: {prev['sql']}. it should be a bit different. Rewrite it.\"})\n",
    "                query,query_embed = self.propose_sql_queries(path_table,SYSTEM_PROMPT)\n",
    " \n",
    "\n",
    "        \n",
    "\n",
    "        return query,query_embed\n",
    "    \n",
    "    def compute_embedding(self,text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute embedding (e.g. using sentence-transformers or OpenAI embeddings).\n",
    "        Return a vector as a list of floats.\n",
    "        \"\"\"\n",
    "        # Example pseudocode:\n",
    "        # from sentence_transformers import SentenceTransformer\n",
    "        embedding = self.model.encode([text], convert_to_tensor=False\n",
    "                        ,batch_size=32,show_progress_bar=False,normalize_embeddings=True)\n",
    "        \n",
    "\n",
    "        return np.array(embedding, dtype=np.float32)\n",
    "lib=SkillLibrary(\"\")\n",
    "Curricu=SQLCurriculumAgent(lib,max_retries=5,transModel=transModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query °1: \n",
      "SELECT Species, AVG(PetalLengthCm) FROM Iris GROUP BY Species; \n",
      "is added.\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[932], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     sql_query,query_embed \u001b[38;5;241m=\u001b[39m \u001b[43mCurricu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropose_sql_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatabase.sqlite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery °\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msql_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mis added.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     lib\u001b[38;5;241m.\u001b[39madd_skill(sql_query, query_embed)\n",
      "Cell \u001b[0;32mIn[931], line 197\u001b[0m, in \u001b[0;36mSQLCurriculumAgent.propose_sql_queries\u001b[0;34m(self, path_table, SYSTEM_PROMPT)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    196\u001b[0m     prev\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_library\u001b[38;5;241m.\u001b[39mskills[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 197\u001b[0m     sec_cond\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(query_embed, prev[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m/\u001b[39m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sec_cond)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sec_cond \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m.6\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    sql_query,query_embed = Curricu.propose_sql_queries('database.sqlite',SYSTEM_PROMPT)\n",
    "    print(f\"Query °{i+1}: \\n{sql_query} \\nis added.\")\n",
    "    lib.add_skill(sql_query, query_embed)\n",
    "    Curricu.messages=None\n",
    "    print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,token=\"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\",device='mps')\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,token=\"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\",device='mps')  # You may want to use bfloat16 and/or move to GPU here\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "                                               )\n",
    "\n",
    "print(tokenizer.decode(tokenized_chat[0]))\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsqlex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
